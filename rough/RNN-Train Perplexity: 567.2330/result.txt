Pride-and-Prejudice-Jane-Austen.txt RNN - Best Params {'embedding_dim': 42, 'hidden_dim': 153, 'learning_rate': 0.0004769520071040205, 'dropout_prob': 0.19028679400620907, 'num_rnn_layers': 2}
Pride-and-Prejudice-Jane-Austen.txt RNN - Test Perplexity: 690.3140
Pride-and-Prejudice-Jane-Austen.txt RNN - Train Perplexity: 567.2330


    # --- Data Loading and Preprocessing ---
    parser = argparse.ArgumentParser(description="Train Neural Language Model")
    
    parser.add_argument("--lm_type", type=str, required=True, choices=["ffnn", "rnn", "lstm"], help="Type of language model to train")
    parser.add_argument("--corpus_path", type=str, required=True, help="Path to the corpus")
    
    # Optimized Hyperparameters
    parser.add_argument("--epochs", type=int, default=50, help="Number of epochs")
    parser.add_argument("--embedding_dim", type=int, default=80, help="Embedding dimension (was 100, optimized for generalization)")
    parser.add_argument("--hidden_dim", type=int, default=250, help="Hidden dimension (was 256, optimized to prevent overfitting)")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch Size (was 128, smaller batches for more stable training)")
    parser.add_argument("--seq_len", type=int, default=20, help="Sequence Length for RNN and LSTM")
    parser.add_argument("--n_gram", type=int, default=3, help="N-gram size for FFNN model")
    parser.add_argument("--min_freq", type=int, default=2, help="Minimum frequency for vocab (was 1, filtering rare words helps generalization)")
    parser.add_argument("--lr", type=float, default=0.008, help="Learning rate (adjusted for better convergence)")

    # Regularization and Optimization
    parser.add_argument("--dropout", type=float, default=2.0, help="Dropout probability (increased for better regularization)")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="L2 regularization (helps prevent overfitting)")